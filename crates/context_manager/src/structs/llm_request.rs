use crate::structs::branch::SystemPrompt;
use crate::structs::context::ChatContext;
use crate::structs::context_agent::AgentRole;
use crate::structs::context_snapshot::LlmContextSnapshot;
use crate::structs::message::InternalMessage;
use uuid::Uuid;

/// Prepared request payload that callers can forward to an LLM adapter.
#[derive(Clone, Debug)]
pub struct PreparedLlmRequest {
    pub context_id: Uuid,
    pub model_id: String,
    pub mode: String,
    pub agent_role: AgentRole,
    pub system_prompt_id: Option<String>,
    pub branch_name: String,
    pub branch_system_prompt: Option<SystemPrompt>,
    pub branch_user_prompt: Option<String>,
    pub messages: Vec<InternalMessage>,
    pub total_messages: usize,
    /// Enhanced system prompt generated by Pipeline (Phase 2.0)
    ///
    /// This is the final system prompt after processing through:
    /// - ToolEnhancementProcessor (adds tool definitions)
    /// - SystemPromptProcessor (assembles fragments)
    ///
    /// If present, this should be used instead of manually enhancing the prompt.
    pub enhanced_system_prompt: Option<String>,
}

impl PreparedLlmRequest {
    fn from_snapshot(snapshot: LlmContextSnapshot) -> Self {
        let messages = snapshot
            .branch
            .messages
            .into_iter()
            .map(|node| node.message)
            .collect();

        Self {
            context_id: snapshot.context_id,
            model_id: snapshot.model_id,
            mode: snapshot.mode,
            agent_role: snapshot.agent_role,
            system_prompt_id: snapshot.system_prompt_id,
            branch_name: snapshot.branch.name,
            branch_system_prompt: snapshot.branch.system_prompt,
            branch_user_prompt: snapshot.branch.user_prompt,
            messages,
            total_messages: snapshot.total_messages,
            enhanced_system_prompt: None, // Will be set by prepare_llm_request if Pipeline is used
        }
    }
}

impl ChatContext {
    /// Prepares a read-only request payload for the current context state.
    ///
    /// Note: This is the legacy synchronous version that does NOT use Pipeline.
    /// For Pipeline-enhanced system prompts, use `prepare_llm_request_async()`.
    pub fn prepare_llm_request(&self) -> PreparedLlmRequest {
        let snapshot = self.llm_snapshot();
        PreparedLlmRequest::from_snapshot(snapshot)
    }

    /// Prepares an LLM request with Pipeline-enhanced system prompt (Phase 2.0)
    ///
    /// This async version:
    /// 1. Gets the base system prompt
    /// 2. Runs SystemPromptProcessor with default enhancers to assemble the final prompt
    ///    - RoleContextEnhancer (adds current role info)
    ///    - ToolEnhancementEnhancer (adds tool definitions)
    ///    - MermaidEnhancementEnhancer (adds Mermaid diagram guidelines if enabled)
    ///    - ContextHintsEnhancer (adds file/tool counts)
    /// 3. Returns PreparedLlmRequest with enhanced_system_prompt set
    ///
    /// # Returns
    /// * `PreparedLlmRequest` with `enhanced_system_prompt` populated
    pub async fn prepare_llm_request_async(&mut self) -> PreparedLlmRequest {
        let mut prepared = self.prepare_llm_request();

        // Build enhanced system prompt using Pipeline processors
        if let Some(enhanced_prompt) = self.build_enhanced_system_prompt().await {
            prepared.enhanced_system_prompt = Some(enhanced_prompt);
        }

        prepared
    }

    /// Build enhanced system prompt using Pipeline processors
    ///
    /// This method:
    /// 1. Gets the base system prompt (from branch only)
    /// 2. Creates a ProcessingContext
    /// 3. Runs ToolEnhancementProcessor
    /// 4. Runs SystemPromptProcessor
    /// 5. Extracts the final system prompt from metadata
    ///
    /// # Returns
    /// * `Some(String)` - The enhanced system prompt (only if branch has a system prompt)
    /// * `None` - If no branch system prompt is configured (caller should use SystemPromptService)
    ///
    /// # Note
    /// This method only enhances branch-level system prompts. If you need to enhance
    /// a prompt from SystemPromptService, the caller (LlmRequestBuilder) should:
    /// 1. Get the prompt from SystemPromptService
    /// 2. Pass it through Pipeline processors manually
    async fn build_enhanced_system_prompt(&mut self) -> Option<String> {
        use crate::pipeline::context::ProcessingContext;
        use crate::pipeline::processors::*;
        use crate::pipeline::traits::MessageProcessor;
        use crate::structs::message::{ContentPart, InternalMessage, MessageType, Role};

        // Only enhance if there's a branch-level system prompt
        // If using system_prompt_id, let the caller (LlmRequestBuilder) handle it
        let base_prompt = if let Some(branch) = self.get_active_branch() {
            if let Some(branch_prompt) = &branch.system_prompt {
                branch_prompt.content.clone()
            } else {
                // No branch prompt - return None so caller can use SystemPromptService
                return None;
            }
        } else {
            // No active branch - return None so caller can use SystemPromptService
            return None;
        };

        // Create a dummy message for ProcessingContext
        let dummy_message = InternalMessage {
            role: Role::User,
            content: vec![ContentPart::text_owned("dummy".to_string())],
            message_type: MessageType::Text,
            ..Default::default()
        };

        // Create processing context
        let mut ctx = ProcessingContext::new(dummy_message, self);

        // Run SystemPromptProcessor with default enhancers
        // This will automatically run all enhancers:
        // - RoleContextEnhancer
        // - ToolEnhancementEnhancer
        // - MermaidEnhancementEnhancer
        // - ContextHintsEnhancer
        let system_prompt_processor =
            system_prompt::SystemPromptProcessor::with_default_enhancers(base_prompt);
        if let Err(e) = system_prompt_processor.process(&mut ctx) {
            log::warn!("SystemPromptProcessor failed: {:?}", e);
            return None;
        }

        // Extract enhanced prompt from metadata
        ctx.get_metadata("final_system_prompt")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
    }
}

impl From<LlmContextSnapshot> for PreparedLlmRequest {
    fn from(value: LlmContextSnapshot) -> Self {
        PreparedLlmRequest::from_snapshot(value)
    }
}
