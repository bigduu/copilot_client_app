# Agent-LLM Protocol Conversion Architecture

## ğŸ“‹ Overview

This document describes the hub-and-spoke protocol conversion system implemented in `crates/agent-llm/src/protocol/`.

## ğŸ¯ Design Goals

1. **Single Source of Truth**: Internal types (`agent_core::Message`, `ToolSchema`) serve as the canonical representation
2. **Minimal Conversion Surface**: N protocols require only 2N conversions (not NÂ²)
3. **Type Safety**: All conversions are verified at compile time
4. **Ergonomic API**: Clean, intuitive traits for conversion
5. **Extensibility**: Easy to add new providers

## ğŸ—ï¸ Architecture

### Hub-and-Spoke Model

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Internal Types (Hub)     â”‚
         â”‚                            â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ agent_core::Message  â”‚  â”‚
         â”‚  â”‚ agent_core::ToolSchemaâ”‚ â”‚
         â”‚  â”‚ agent_core::ToolCall â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²    â”‚
          FromProviderâ”‚    â”‚ToProvider
                      â”‚    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Provider Types  â”‚  â”‚  Provider Types  â”‚
    â”‚  (Spoke 1)       â”‚  â”‚  (Spoke 2)       â”‚
    â”‚                  â”‚  â”‚                  â”‚
    â”‚  OpenAI API      â”‚  â”‚  Anthropic API   â”‚
    â”‚  ChatMessage     â”‚  â”‚  AnthropicMessageâ”‚
    â”‚  Tool            â”‚  â”‚  AnthropicTool   â”‚
    â”‚  ToolCall        â”‚  â”‚  ToolUse         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Conversion Flow

```
External Format A â†’ Internal Format â†’ External Format B
      (Spoke 1)      (Hub)              (Spoke 2)
```

## ğŸ“¦ Module Structure

```
crates/agent-llm/src/protocol/
â”œâ”€â”€ mod.rs          # Core traits: FromProvider, ToProvider
â”œâ”€â”€ errors.rs       # ProtocolError enum
â”œâ”€â”€ openai.rs       # OpenAI protocol implementation
â”œâ”€â”€ anthropic.rs    # Anthropic protocol implementation
â””â”€â”€ (future)
    â”œâ”€â”€ gemini.rs   # Future: Google Gemini
    â””â”€â”€ mistral.rs  # Future: Mistral AI
```

## ğŸ”‘ Core Traits

### FromProvider (Spoke â†’ Hub)

Converts provider-specific types to internal types.

```rust
pub trait FromProvider<T>: Sized {
    fn from_provider(value: T) -> ProtocolResult<Self>;
}
```

**Example:**
```rust
impl FromProvider<OpenAIChatMessage> for Message {
    fn from_provider(msg: OpenAIChatMessage) -> ProtocolResult<Self> {
        // OpenAI â†’ Internal conversion logic
    }
}
```

### ToProvider (Hub â†’ Spoke)

Converts internal types to provider-specific types.

```rust
pub trait ToProvider<T>: Sized {
    fn to_provider(&self) -> ProtocolResult<T>;
}
```

**Example:**
```rust
impl ToProvider<AnthropicMessage> for Message {
    fn to_provider(&self) -> ProtocolResult<AnthropicMessage> {
        // Internal â†’ Anthropic conversion logic
    }
}
```

### Batch Conversion

```rust
pub trait ToProviderBatch<T>: Sized {
    fn to_provider_batch(&self) -> ProtocolResult<Vec<T>>;
}

// Implemented for Vec<Message>
impl ToProviderBatch<OpenAIChatMessage> for Vec<Message> { /* ... */ }
```

## ğŸ”„ Supported Conversions

### OpenAI Protocol

| Direction | Type Mapping |
|-----------|-------------|
| OpenAI â†’ Internal | `ChatMessage` â†’ `Message` |
| Internal â†’ OpenAI | `Message` â†’ `ChatMessage` |
| OpenAI â†’ Internal | `Tool` â†’ `ToolSchema` |
| Internal â†’ OpenAI | `ToolSchema` â†’ `Tool` |
| OpenAI â†’ Internal | `ToolCall` â†’ `ToolCall` |
| Internal â†’ OpenAI | `ToolCall` â†’ `ToolCall` |

**Special Handling:**
- Content parts (text + images) are flattened to text
- Role enum values map directly

### Anthropic Protocol

| Direction | Type Mapping |
|-----------|-------------|
| Anthropic â†’ Internal | `AnthropicMessage` â†’ `Message` |
| Internal â†’ Anthropic | `Message` â†’ `AnthropicMessage` |
| Anthropic â†’ Internal | `AnthropicTool` â†’ `ToolSchema` |
| Internal â†’ Anthropic | `ToolSchema` â†’ `AnthropicTool` |

**Special Handling:**
- System messages are extracted to top-level `system` field
- Tool calls become `tool_use` blocks in content
- Tool results become `tool_result` blocks in user messages
- Content blocks vs text string

## ğŸ­ Protocol-Specific Behaviors

### OpenAI

```rust
// OpenAI keeps everything in messages array
let openai_request = vec![
    ChatMessage { role: System, content: "You are helpful", ... },
    ChatMessage { role: User, content: "Hello", ... },
];
```

### Anthropic

```rust
// Anthropic extracts system to top level
let anthropic_request = AnthropicRequest {
    system: Some("You are helpful"),
    messages: vec![
        AnthropicMessage { role: User, content: ..., ... },
    ],
};
```

## ğŸ§ª Testing Strategy

### Unit Tests

Each protocol module includes comprehensive tests:

```rust
#[test]
fn test_openai_to_internal_simple_message() { /* ... */ }

#[test]
fn test_roundtrip_conversion() {
    // Internal â†’ Provider â†’ Internal should preserve data
    let original = Message::user("Hello");
    let provider_msg: OpenAIChatMessage = original.to_provider().unwrap();
    let roundtrip: Message = Message::from_provider(provider_msg).unwrap();

    assert_eq!(roundtrip.role, original.role);
    assert_eq!(roundtrip.content, original.content);
}
```

### Integration Tests

Run with:
```bash
cargo test -p agent-llm --lib protocol
```

## ğŸ”® Adding a New Provider

To add support for a new LLM provider:

### Step 1: Define Provider Types

```rust
// protocol/newprovider.rs
pub struct NewProviderMessage {
    pub role: String,
    pub content: String,
    // ...
}
```

### Step 2: Implement FromProvider

```rust
impl FromProvider<NewProviderMessage> for Message {
    fn from_provider(msg: NewProviderMessage) -> ProtocolResult<Self> {
        let role = match msg.role.as_str() {
            "user" => Role::User,
            "assistant" => Role::Assistant,
            // ...
        };

        Ok(Message {
            role,
            content: msg.content,
            // ...
        })
    }
}
```

### Step 3: Implement ToProvider

```rust
impl ToProvider<NewProviderMessage> for Message {
    fn to_provider(&self) -> ProtocolResult<NewProviderMessage> {
        let role = match self.role {
            Role::User => "user".to_string(),
            Role::Assistant => "assistant".to_string(),
            // ...
        };

        Ok(NewProviderMessage {
            role,
            content: self.content.clone(),
            // ...
        })
    }
}
```

### Step 4: Add Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_newprovider_to_internal() { /* ... */ }

    #[test]
    fn test_internal_to_newprovider() { /* ... */ }

    #[test]
    fn test_roundtrip_conversion() { /* ... */ }
}
```

### Step 5: Export in mod.rs

```rust
// protocol/mod.rs
pub mod newprovider;
pub use newprovider::NewProviderProtocol;
```

## ğŸ“Š Performance Considerations

- **Zero-cost abstractions**: Traits are monomorphized at compile time
- **No runtime dispatch**: All conversions resolved statically
- **Minimal allocations**: Most conversions are field-by-field copies
- **Error handling**: `ProtocolResult<T>` avoids panics

## ğŸ”’ Error Handling

```rust
pub enum ProtocolError {
    Serialization(serde_json::Error),
    InvalidRole(String),
    InvalidContent(String),
    MissingField(String),
    UnsupportedFeature { feature: String, protocol: String },
    InvalidToolCall(String),
    InvalidStreamChunk(String),
    Conversion(String),
}
```

## ğŸ“š Usage Patterns

### Pattern 1: Direct Conversion

```rust
let internal = Message::user("Hello");
let openai: OpenAIChatMessage = internal.to_provider()?;
```

### Pattern 2: Cross-Protocol

```rust
// OpenAI â†’ Internal â†’ Anthropic
let openai_msg = /* ... */;
let internal: Message = Message::from_provider(openai_msg)?;
let anthropic: AnthropicMessage = internal.to_provider()?;
```

### Pattern 3: Batch Processing

```rust
let messages = vec![
    Message::system("Be helpful"),
    Message::user("Hello"),
];

let openai_messages: Vec<OpenAIChatMessage> = messages.to_provider_batch()?;
```

## ğŸ“ Best Practices

1. **Store as Internal**: Always store messages as `agent_core::Message` in your application
2. **Convert at Boundaries**: Convert to provider types only at API boundaries
3. **Handle Errors**: Don't unwrap conversion errors in production code
4. **Test Round-Trips**: Ensure data preservation through conversions
5. **Document Differences**: Note protocol-specific behaviors in comments

## ğŸ”— Related Files

- `PROTOCOL_GUIDE.md` - User-facing usage guide
- `protocol/mod.rs` - Trait definitions
- `protocol/openai.rs` - OpenAI implementation
- `protocol/anthropic.rs` - Anthropic implementation

## ğŸ¤ Contributing

When adding new conversions:

1. Follow the existing pattern in `openai.rs` or `anthropic.rs`
2. Add comprehensive tests for all conversion directions
3. Document protocol-specific behaviors
4. Update this architecture document
5. Add examples to `PROTOCOL_GUIDE.md`

## ğŸ“œ History

- **Initial Design**: Hub-and-spoke architecture chosen to avoid NÂ² conversion matrix
- **Rationale**: With N providers, we need only 2N conversions (not N(N-1))
- **Migration**: Old conversion code in `providers/common/openai_compat.rs` and `providers/anthropic/mod.rs` is being migrated to this new system
