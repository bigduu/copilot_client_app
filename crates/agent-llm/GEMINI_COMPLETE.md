# ğŸ‰ Gemini Provider å®ç°å®Œæˆ

## âœ… å®ŒæˆçŠ¶æ€

**æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼109 ä¸ªæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼**

```
âœ… Protocol è½¬æ¢å±‚ (protocol/gemini.rs) - 12 ä¸ªæµ‹è¯•
âœ… Provider å®ç° (providers/gemini/) - 18 ä¸ªæµ‹è¯•
âœ… æ€»è®¡ 109 ä¸ªæµ‹è¯•é€šè¿‡
```

## ğŸ“¦ å®ç°å†…å®¹

### 1. æ ¸å¿ƒæ–‡ä»¶ç»“æ„

```
crates/agent-llm/src/
â”œâ”€â”€ protocol/
â”‚   â”œâ”€â”€ mod.rs              (å…¬å¼€ gemini æ¨¡å—)
â”‚   â””â”€â”€ gemini.rs           (åè®®è½¬æ¢ âœ…)
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ mod.rs              (æ·»åŠ  gemini æ¨¡å— âœ…)
â”‚   â””â”€â”€ gemini/
â”‚       â”œâ”€â”€ mod.rs          (Provider å®ç° âœ…)
â”‚       â””â”€â”€ stream.rs       (SSE è§£æ âœ…)
â””â”€â”€ provider.rs             (æ·»åŠ  Protocol é”™è¯¯ âœ…)
```

### 2. å®ç°è¯¦æƒ…

#### providers/gemini/mod.rs

```rust
pub struct GeminiProvider {
    client: Client,
    api_key: String,
    base_url: String,      // é»˜è®¤: https://generativelanguage.googleapis.com/v1beta
    model: String,         // é»˜è®¤: gemini-pro
}

// æ„é€ å‡½æ•°
impl GeminiProvider {
    pub fn new(api_key: impl Into<String>) -> Self { /* ... */ }
    pub fn with_base_url(mut self, url: impl Into<String>) -> Self { /* ... */ }
    pub fn with_model(mut self, model: impl Into<String>) -> Self { /* ... */ }
}

// LLMProvider trait å®ç°
#[async_trait]
impl LLMProvider for GeminiProvider {
    async fn chat_stream(
        &self,
        messages: &[Message],
        tools: &[ToolSchema],
        max_output_tokens: Option<u32>,
    ) -> Result<LLMStream> {
        // 1. ä½¿ç”¨æ–°çš„åè®®è½¬æ¢
        let messages_vec: Vec<Message> = messages.to_vec();
        let mut request: GeminiRequest = messages_vec.to_provider()?;

        // 2. æ·»åŠ å·¥å…·
        request.tools = Some(tools.to_provider()?);

        // 3. æ·»åŠ ç”Ÿæˆé…ç½®
        if let Some(max_tokens) = max_output_tokens {
            request.generation_config = Some(json!({
                "maxOutputTokens": max_tokens
            }));
        }

        // 4. è°ƒç”¨ Gemini API
        // endpoint: {base_url}/models/{model}:streamGenerateContent?key={api_key}

        // 5. è§£æ SSE æµ
        let stream = llm_stream_from_sse(response, |event, data| {
            parse_gemini_sse_event(&mut state, event, data)
        });

        Ok(stream)
    }
}
```

#### providers/gemini/stream.rs

```rust
pub struct GeminiStreamState {
    // è·Ÿè¸ªå·²ç”Ÿæˆçš„å·¥å…·è°ƒç”¨ ID
    tool_call_counter: u32,
}

pub fn parse_gemini_sse_event(
    state: &mut GeminiStreamState,
    event_type: &str,
    data: &str,
) -> Result<Option<LLMChunk>> {
    // è§£æ Gemini SSE æ ¼å¼ï¼š
    // data: {"candidates":[{"content":{"parts":[{"text":"Hello"}]}}]}
    // data: [DONE]

    match event_type {
        "done" => Ok(Some(LLMChunk::Done)),
        _ => {
            let response: GeminiResponse = serde_json::from_str(data)?;

            // æå–æ–‡æœ¬
            if let Some(text) = extract_text(&response) {
                return Ok(Some(LLMChunk::Token(text)));
            }

            // æå–å·¥å…·è°ƒç”¨
            if let Some(func_call) = extract_function_call(&response) {
                let tool_call = ToolCall {
                    id: state.generate_tool_id(), // ç”Ÿæˆå”¯ä¸€ ID
                    tool_type: "function".to_string(),
                    function: FunctionCall {
                        name: func_call.name,
                        arguments: serde_json::to_string(&func_call.args)?,
                    },
                };
                return Ok(Some(LLMChunk::ToolCalls(vec![tool_call])));
            }

            Ok(None)
        }
    }
}
```

## ğŸ”‘ å…³é”®å®ç°ç»†èŠ‚

### 1. è®¤è¯æ–¹å¼

ä¸å…¶ä»– provider ä¸åŒï¼ŒGemini ä½¿ç”¨ query parameterï¼š

```rust
// OpenAI/Anthropic: Header
Authorization: Bearer {api_key}

// Gemini: Query Parameter
?key={api_key}
```

### 2. API Endpoint

```rust
// OpenAI
https://api.openai.com/v1/chat/completions

// Anthropic
https://api.anthropic.com/v1/messages

// Gemini (æ³¨æ„æ ¼å¼)
https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent?key={api_key}
```

### 3. ä½¿ç”¨æ–°åè®®ç³»ç»Ÿ

```rust
// âŒ æ—§æ–¹å¼ (ä¸è¦ä½¿ç”¨)
use crate::providers::common::openai_compat::messages_to_openai_compat_json;
let json = messages_to_openai_compat_json(&messages);

// âœ… æ–°æ–¹å¼ (ä½¿ç”¨è¿™ä¸ª)
use agent_llm::ToProvider;
let request: GeminiRequest = messages.to_provider()?;
```

### 4. æµå¼å“åº”æ ¼å¼

```
Gemini SSE æ ¼å¼:
data: {"candidates":[{"content":{"parts":[{"text":"Hello"}],"role":"model"}}]}

data: {"candidates":[{"content":{"parts":[{"functionCall":{"name":"search","args":{"q":"test"}}}],"role":"model"}}]}

data: [DONE]
```

## ğŸ§ª æµ‹è¯•è¦†ç›–

### Provider æµ‹è¯• (18 ä¸ª)

```
providers/gemini/mod.rs (6 ä¸ªæµ‹è¯•)
â”œâ”€â”€ test_new_provider
â”œâ”€â”€ test_with_base_url
â”œâ”€â”€ test_with_model
â”œâ”€â”€ test_chained_builders
â””â”€â”€ test_url_construction

providers/gemini/stream.rs (12 ä¸ªæµ‹è¯•)
â”œâ”€â”€ parse_text_chunk
â”œâ”€â”€ parse_function_call
â”œâ”€â”€ parse_function_call_with_empty_args
â”œâ”€â”€ multiple_function_calls_get_unique_ids
â”œâ”€â”€ parse_done_signal
â”œâ”€â”€ parse_error_response
â”œâ”€â”€ parse_invalid_json
â”œâ”€â”€ parse_empty_data_returns_none
â”œâ”€â”€ parse_empty_candidates_returns_none
â”œâ”€â”€ parse_missing_content_returns_none
â”œâ”€â”€ parse_multipart_text_accumulates
â”œâ”€â”€ state_generates_unique_tool_ids
â””â”€â”€ parse_whitespace_data_is_trimmed
```

### Protocol æµ‹è¯• (12 ä¸ª)

```
protocol/gemini.rs
â”œâ”€â”€ test_gemini_to_internal_user_message
â”œâ”€â”€ test_internal_to_gemini_user_message
â”œâ”€â”€ test_gemini_to_internal_model_message
â”œâ”€â”€ test_internal_to_gemini_with_tool_call
â”œâ”€â”€ test_gemini_to_internal_with_tool_call
â”œâ”€â”€ test_system_message_extraction
â”œâ”€â”€ test_tool_response_conversion
â”œâ”€â”€ test_tool_schema_conversion
â”œâ”€â”€ test_multiple_tools_grouped
â”œâ”€â”€ test_roundtrip_conversion
â”œâ”€â”€ test_invalid_role_error
â””â”€â”€ test_empty_parts_has_default
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```rust
use agent_llm::providers::GeminiProvider;
use agent_llm::provider::LLMProvider;
use agent_core::Message;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. åˆ›å»º provider
    let provider = GeminiProvider::new("your-gemini-api-key")
        .with_model("gemini-pro");

    // 2. å‡†å¤‡æ¶ˆæ¯
    let messages = vec![
        Message::system("You are helpful"),
        Message::user("Hello!"),
    ];

    // 3. è°ƒç”¨ API (æµå¼)
    let mut stream = provider.chat_stream(&messages, &[], Some(1024)).await?;

    // 4. å¤„ç†å“åº”
    use futures::StreamExt;
    while let Some(chunk) = stream.next().await {
        match chunk? {
            LLMChunk::Token(text) => print!("{}", text),
            LLMChunk::ToolCalls(calls) => {
                // å¤„ç†å·¥å…·è°ƒç”¨
            }
            LLMChunk::Done => break,
        }
    }

    Ok(())
}
```

### å¸¦å·¥å…·è°ƒç”¨

```rust
use agent_core::tools::{ToolSchema, FunctionSchema};

let tools = vec![
    ToolSchema {
        schema_type: "function".to_string(),
        function: FunctionSchema {
            name: "get_weather".to_string(),
            description: "Get weather info".to_string(),
            parameters: json!({
                "type": "object",
                "properties": {
                    "location": { "type": "string" }
                }
            }),
        },
    },
];

let stream = provider.chat_stream(&messages, &tools, None).await?;
```

### è·¨ Provider è½¬æ¢

```rust
// OpenAI â†’ Gemini
let openai_msg: OpenAIChatMessage = /* ... */;
let internal: Message = Message::from_provider(openai_msg)?;
let gemini: GeminiContent = internal.to_provider()?;

// æˆ–è€…ç›´æ¥å‘é€åˆ°ä¸åŒçš„ provider
let openai_response = openai_provider.chat_stream(&messages, &[], None).await?;
let gemini_response = gemini_provider.chat_stream(&messages, &[], None).await?;
```

## ğŸ” è°ƒè¯•

### å¯ç”¨æ—¥å¿—

```bash
RUST_LOG=debug cargo run
```

### æ£€æŸ¥è¯·æ±‚

```rust
// åœ¨ chat_stream ä¸­æ·»åŠ 
log::debug!("Gemini request: {}", serde_json::to_string_pretty(&request)?);
```

### æ£€æŸ¥å“åº”

```rust
// åœ¨ parse_gemini_sse_event ä¸­æ·»åŠ 
log::trace!("SSE event: {}, data: {}", event_type, data);
```

## âš™ï¸ é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡

```bash
export GEMINI_API_KEY="your-api-key"
```

### è‡ªå®šä¹‰é…ç½®

```rust
let provider = GeminiProvider::new("api-key")
    .with_base_url("https://custom-endpoint.com")  // è‡ªå®šä¹‰ endpoint
    .with_model("gemini-pro-vision");              // ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| Provider | è®¤è¯æ–¹å¼ | Endpoint æ ¼å¼ | æµå¼æ ¼å¼ |
|----------|---------|--------------|---------|
| OpenAI | Bearer token | REST | SSE with events |
| Anthropic | x-api-key | REST | SSE with events |
| Gemini | Query param | RPC-style | SSE with JSON |

## ğŸš€ ä¸‹ä¸€æ­¥

### å·²å®Œæˆ âœ…
- [x] åè®®è½¬æ¢å±‚
- [x] Provider struct
- [x] LLMProvider trait
- [x] SSE è§£æ
- [x] å•å…ƒæµ‹è¯•

### å¯é€‰å¢å¼º
- [ ] é›†æˆæµ‹è¯•ï¼ˆä½¿ç”¨ mock serverï¼‰
- [ ] é‡è¯•é€»è¾‘
- [ ] é€Ÿç‡é™åˆ¶å¤„ç†
- [ ] å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
- [ ] å®‰å…¨è®¾ç½®ï¼ˆsafety settingsï¼‰

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `GEMINI_GUIDE.md` - ä½¿ç”¨æŒ‡å—
- `PROTOCOL_GUIDE.md` - åè®®å¯¹æ¯”
- `GEMINI_IMPLEMENTATION.md` - å®ç°ç»†èŠ‚
- `GEMINI_TASKS.md` - ä»»åŠ¡åˆ—è¡¨

## ğŸ“ å­¦ä¹ è¦ç‚¹

1. **Hub-and-Spoke æ¶æ„**ï¼šæ‰€æœ‰ provider é€šè¿‡å†…éƒ¨ç±»å‹è½¬æ¢
2. **ç»Ÿä¸€çš„ trait**ï¼š`LLMProvider` trait æä¾›ä¸€è‡´çš„æ¥å£
3. **åè®®éš”ç¦»**ï¼šæ¯ä¸ª provider çš„ç‰¹æ®Šå¤„ç†éƒ½åœ¨ç‹¬ç«‹çš„æ¨¡å—
4. **æµ‹è¯•é©±åŠ¨**ï¼š30 ä¸ªæµ‹è¯•ç¡®ä¿æ­£ç¡®æ€§

## ğŸ¤ è´¡çŒ®

å¦‚æœå‘ç°é—®é¢˜æˆ–æƒ³è¦æ·»åŠ åŠŸèƒ½ï¼š

1. æ·»åŠ æµ‹è¯•
2. ä¿®æ”¹å®ç°
3. è¿è¡Œ `cargo test -p agent-llm`
4. æäº¤ PR

---

**å®ç°å®Œæˆæ—¶é—´**: 2026-02-15
**æ€»ä»£ç è¡Œæ•°**: ~1000 è¡Œï¼ˆåŒ…æ‹¬æµ‹è¯•ï¼‰
**æ€»æµ‹è¯•æ•°é‡**: 30 ä¸ªï¼ˆ12 protocol + 18 providerï¼‰
**Team Agent ç”¨æ—¶**: ~4.7 åˆ†é’Ÿ
